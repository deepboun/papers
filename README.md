# Papers 
List of papers that we will or did discuss during the sessions.

## Upcoming

| Date        | Title           | Presenter  |
| ------------- |:-------------:| -----:|
| [02/02/2017](README.md#02022017-scroll-understanding-neural-networks-through-representation-erasure)      | Understanding Neural Networks through Representation Erasure | Onur Gungor |
| TBA | Learning to learn by gradient descent by gradient descent | Arda Celebi |
| TBA | A Convolutional Neural Network for Modelling Sentences | Çağıl Sönmez |

## Past

| Date        | Title           | Presenter  |
| ------------- |:-------------:| -----:|
| [26/01/2017](README.md#26012017scroll-collaborative-deep-learning-for-recommender-systems)      | Collaborative Deep Learning For Recommender Systems | Mine Öğretir |
| [19/01/2017](README.md#19012017scroll-relation-extraction-perspective-from-convolutional-neural-networks) | Relation Extraction: Perspective from Convolutional Neural Networks | Çağıl Sönmez |



## Detailed List of Papers Upcoming

### (02/02/2017) :scroll: Understanding Neural Networks through Representation Erasure

  **Citation** :
  
  **Presenter** : Onur Gungor

  **In short** : 'In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words.'

  **Paper** : https://arxiv.org/abs/1612.08220
  
  **Date**: 2 February 2017
  
  **Reference**: Li, J., Monroe, W., & Jurafsky, D. (2016). Understanding Neural Networks through Representation Erasure. arXiv preprint arXiv:1612.08220.
  

### (TBA):scroll: Learning to learn by gradient descent by gradient descent 

  **Citation** : Learning to learn by gradient descent by gradient descent by Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas; NIPS 2016

  **Presenter** : Arda Celebi

  **In short** : Optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way.

  **Paper** : https://arxiv.org/abs/1606.04474
  
  **Github** : https://github.com/deepmind/learning-to-learn
  
  **Date**: TBA
  
  
### (TBA):scroll: **[A Convolutional Neural Network for Modelling Sentences](http://aclanthology.info/papers/a-convolutional-neural-network-for-modelling-sentences)**

  **Citation:** Nal Kalchbrenner, Edward Grefenstette and Phil Blunsom. 2014. A Convolutional Neural Network for Modelling Sentences. In Proceedings of ACL 2014.

  **One sentence from Abstract:** We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. 

  **Presenter:** Çağıl Sönmez

  **Link:** http://aclanthology.info/papers/a-convolutional-neural-network-for-modelling-sentences
  
  **More info:** [Code](http://phd.nal.co/DCNN) [Theano implementation](https://github.com/FredericGodin/DynamicCNN) [A Sentiment Analysis tool written in Theono+Tornado](https://github.com/xiaohan2012/twitter-sent-dnn)


## Detailed List of Papers Past

### (19/01/2017):scroll: **[Relation Extraction: Perspective from Convolutional Neural Networks](http://aclanthology.info/papers/relation-extraction-perspective-from-convolutional-neural-networks)**

  **Citation:** Thien Huu Nguyen and Ralph Grishman. 2015a. Relation extraction: Perspective from convolutional neural networks. In The NAACL Workshop on Vector Space Modeling for NLP (VSM).

  **One sentence from Abstract:** In this work, we depart from these traditional approaches with complicated feature engineering by introducing a convolutional neural network for relation extraction that automatically learns features from sentences and minimizes the dependence on external toolkits and resources. Our model takes advantages of multiple window sizes for filters and pre-trained word embeddings as an initializer on a non-static architecture to improve the performance. We emphasize the relation extraction problem with an unbalanced corpus.

  **Link:** http://aclanthology.info/papers/relation-extraction-perspective-from-convolutional-neural-networks
  
  **More info:** [Implementation](https://github.com/hadyelsahar/CNN-RelationExtraction) 
  
### (26/01/2017):scroll: **[Collaborative Deep Learning For Recommender Systems](http://dl.acm.org/citation.cfm?id=2783273)**

  **Citation:** H. Wang, N. Wang, and D. Yeung. Collaborative deep learning for recommender systems. In KDD, 2015.

  **From Abstract:** We generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix.

  **Link:** http://dl.acm.org/citation.cfm?id=2783273
  
  **More info:** [Related Material](http://www.wanghao.in/publication.html) 
